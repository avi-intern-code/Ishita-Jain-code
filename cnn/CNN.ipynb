{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b10e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn: convolutional neural network\n",
    "# netwok architecture for deep learning \n",
    "# learns directly from images\n",
    "# typical neural network: every input connect to hidden layer\n",
    "# cnn: only small region of input layer connect to hidden layer\n",
    "# these regions are local receptive fields\n",
    "# weights & bias value same for all hidden neurons in a given layer\n",
    "# relu (rectified linear unit) is an activation func\n",
    "# it take output of a neuron and maps it to highest +ve value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution: i/p, kernel (filters), n no. of kernels, o/p, strid\n",
    "# non linear activaton layer: brings non linearity\n",
    "#pooling: takes i/p and convert it in pixels\n",
    "# fully connected layer: create bunch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist #28x28 images of handwritten digits 0-9\n",
    "(x_train,y_train), (x_test,y_test)= mnist.load_data()\n",
    "# normalize scale values \n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test=tf.keras.utils.normalize(x_test,axis=1)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu)) # 128 is the no. of neurons\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax)) # softmax for probability distribution\n",
    "\n",
    "model.compile(optimizer= 'adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babda76",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb83a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#print(x_train[0])\n",
    "plt.imshow(x_train[0], cmap = plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('epic_num_reader.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932aeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model=tf.keras.models.load_model('epic_num_reader.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = new_model.predict([x_test])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.argmax(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f094bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "data = \"C:/Users/Ishita/tutorial/cnn/PetImages\"\n",
    "cg = ['dog', 'cat']\n",
    "\n",
    "for c in cg:\n",
    "    path = os.path.join(data,c) # path to cats or dogs directory\n",
    "    for img in os.listdir(path):\n",
    "        img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(img_arr, cmap='gray')\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17118620",
   "metadata": {},
   "outputs": [],
   "source": [
    "size=50\n",
    "new=cv2.resize(img_arr, (size, size))\n",
    "plt.imshow(new, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186a0a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t=[]\n",
    "def td():\n",
    "    for c in cg:\n",
    "        path = os.path.join(data,c) # path to cats or dogs directory\n",
    "        n=cg.index(c)\n",
    "        for img in os.listdir(path):\n",
    "                print(os.path.join(path, img))\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                new=cv2.resize(img_arr, (size,size))\n",
    "                t.append([new,n])\n",
    "td()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d9f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b193ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in t[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6ea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28758da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in t:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "    \n",
    "x=np.array(x).reshape(-1, size, size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "po = open('x.pickle','wb')\n",
    "pickle.dump(x, po)\n",
    "po.close()\n",
    "\n",
    "po = open('y.pickle','wb')\n",
    "pickle.dump(y, po)\n",
    "po.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi= open('x.pickle', 'rb')\n",
    "x=pickle.load(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3910e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "\n",
    "x = pickle.load(open('x.pickle', 'rb'))\n",
    "y = pickle.load(open('y.pickle', 'rb'))\n",
    "\n",
    "x = np.array(x/255.0)\n",
    "y=np.array(y)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = x.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(Conv2D(64, (3,3))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x,y,batch_size=32, epochs=3, validation_split=0.3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5dd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "name = 'cats-vs-dogs-{}'.format(int(time.time()))\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(name))\n",
    "\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "x = pickle.load(open('x.pickle', 'rb'))\n",
    "y = pickle.load(open('y.pickle', 'rb'))\n",
    "\n",
    "x = np.array(x/255.0)\n",
    "y=np.array(y)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,3), input_shape = x.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(Conv2D(64, (3,3))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "model.add(Flatten()) # convert 3d feature map to 1d feature vector\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x,y,batch_size=32, epochs=3, validation_split=0.1, callbacks = [tensorboard]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "dense_layers=[0,1,2]\n",
    "layer_sizes=[32,64,128]\n",
    "conv_layers=[1,2,3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            name = '{}-cov-{}-nodes-{}-dense-{}'.format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb397a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "name = 'cats-vs-dogs-{}'.format(int(time.time()))\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(name))\n",
    "\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "x = pickle.load(open('x.pickle', 'rb'))\n",
    "y = pickle.load(open('y.pickle', 'rb'))\n",
    "\n",
    "x = np.array(x/255.0)\n",
    "y=np.array(y)\n",
    "\n",
    "dense_layers=[2]\n",
    "layer_sizes=[64]\n",
    "conv_layers=[3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            name = '{}-cov-{}-nodes-{}-dense-{}'.format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            print(name)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3,3), input_shape = x.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "            \n",
    "            for l in range(conv_layer-1):\n",
    "                model.add(Conv2D(layer_size , (3,3))) \n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2))) \n",
    "\n",
    "            model.add(Flatten())  # convert 3d feature map to 1d feature vector\n",
    "            for l in range (dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', \n",
    "                          optimizer='adam', \n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            model.fit(x,y,batch_size=32, epochs=3, validation_split=0.1, callbacks = [tensorboard]) \n",
    "model.save('64x3-CNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6359252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "cg=['dog', 'cat']\n",
    "\n",
    "def prepare(filepath):\n",
    "    size = 70\n",
    "    img_arr = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new=cv2.resize(img_arr, (size, size))\n",
    "    return new.reshape(-1, size, size)\n",
    "\n",
    "model = tf.keras.models.load_model(\"64x3-CNN.model\")\n",
    "prediction = model.predict([prepare('C://Users//Ishita//tutorial//opencv-master//samples//data//lena.jpg')])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([prepare('lena.jpg')])\n",
    "print(cg[int(prediction[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781eb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "for img in test:\n",
    "    image=Image.open(img)\n",
    "    image_np=load_image_into_numpy_array(image)\n",
    "    \n",
    "while True:\n",
    "    ret, image_np=cap.read()\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image_np)\n",
    "    plt.show\n",
    "    \n",
    "    cv2.imshow('image', cv2.resize(image_np,(800,600)))\n",
    "    if cv2.waitKey(27):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this URL to the API endpoint of deployed model.\n",
    "MAXIMO_VISUAL_INSPECTION_API_URL = \"https://ny1.ptopenlab.com/powerai-vision-ny/api/dlapis/your-guid-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479060c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN = False  # Wipe out saved files when this is true (else reuse for speed)\n",
    "input_video= cv2.imread(\"C://Users//Ishita//tutorial//opencv-master//samples//data//test_video.mp4\")  # The input video\n",
    "START_LINE = 0  # If start line is > 0, cars won't be added until below the line (try 200)\n",
    "FRAMES_DIR = \"frames\"  # Output dir to hold/cache the original frames\n",
    "OUTPUT_DIR = \"output\"  # Output dir to hold the annotated frames\n",
    "SAMPLING = 10  # Classify every n frames (use tracking in between)\n",
    "CONFIDENCE = 0.80  # Confidence threshold to filter iffy objects\n",
    "\n",
    "# OpenCV colors are (B, G, R) tuples -- RGB in reverse\n",
    "WHITE = (255, 255, 255)\n",
    "YELLOW = (66, 244, 238)\n",
    "GREEN = (80, 220, 60)\n",
    "LIGHT_CYAN = (255, 255, 224)\n",
    "DARK_BLUE = (139, 0, 0)\n",
    "GRAY = (128, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "from IPython.display import clear_output, Image, display\n",
    "import requests\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "print(\"Warning: Certificates not verified!\")\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small example video\n",
    "input_video = input_video.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d76f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN:\n",
    "    if os.path.isdir(FRAMES_DIR):\n",
    "        shutil.rmtree(FRAMES_DIR)\n",
    "    if os.path.isdir(OUTPUT_DIR):\n",
    "        shutil.rmtree(OUTPUT_DIR)\n",
    "\n",
    "if not os.path.isdir(FRAMES_DIR):\n",
    "    os.mkdir(FRAMES_DIR)\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each frame is saved as an individual JPEG file for later use.\n",
    "#if os.path.isfile(input_video):\n",
    "video_capture = cv2.VideoCapture(input_video)\n",
    "#else:\n",
    " #   raise Exception(\"File %s doesn't exist!\" % input_video)\n",
    "\n",
    "total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"Frame count estimate is %d\" % total_frames)\n",
    "\n",
    "num = 0\n",
    "while video_capture.get(cv2.CAP_PROP_POS_FRAMES) < video_capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "    success, image = video_capture.read()\n",
    "    if success:\n",
    "        num = int(video_capture.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        print(\"Writing frame {num} of {total_frames}\".format(\n",
    "            num=num, total_frames=total_frames), end=\"\\r\")\n",
    "        cv2.imwrite('{frames_dir}/frame_{num:05d}.jpg'.format(\n",
    "            frames_dir=FRAMES_DIR, num=num), image)\n",
    "    else:\n",
    "        # TODO: If this happens, we need to add retry code\n",
    "        raise Exception('Error writing frame_{num:05d}.jpg'.format(\n",
    "            num=int(video_capture.get(cv2.CAP_PROP_POS_FRAMES))))\n",
    "\n",
    "print(\"\\nWrote {num} frames\".format(num=num))\n",
    "\n",
    "FRAME_FPS = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "FRAME_WIDTH = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "FRAME_HEIGHT = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "ROI_YMAX = int(round(FRAME_HEIGHT * 0.75))  # Bottom quarter = finish line\n",
    "\n",
    "print(\"Frame Dimensions: %sx%s\" % (FRAME_WIDTH, FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d727130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper/wrapper to call Maximo Visual Insights and return the inference result.\n",
    "s = requests.Session()\n",
    "\n",
    "\n",
    "def detect_objects(filename):\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        # WARNING! verify=False is here to allow an untrusted cert!\n",
    "        r = s.post(MAXIMO_VISUAL_INSPECTION_API_URL,\n",
    "                   files={'files': (filename, f)},\n",
    "                   verify=False)\n",
    "\n",
    "    return r.status_code, json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc, jsonresp = detect_objects('frames/frame_00100.jpg')\n",
    "\n",
    "print(\"rc = %d\" % rc)\n",
    "print(\"jsonresp: %s\" % jsonresp)\n",
    "if 'classified' in jsonresp:\n",
    "    print(\"Got back %d objects\" % len(jsonresp['classified']))\n",
    "print(json.dumps(jsonresp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b00143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize requests, storing them in a \"tracking_results\" dict\n",
    "\n",
    "try:\n",
    "    with open('frames/frame-data-newmodel.json') as existing_results:\n",
    "        tracking_results = json.load(existing_results)\n",
    "except Exception:\n",
    "    # Any fail to read existing results means we start over\n",
    "    tracking_results = {}\n",
    "\n",
    "print(\"Sampling every %sth frame\" % SAMPLING)\n",
    "i = 0\n",
    "cache_used = 0\n",
    "sampled = 0\n",
    "for filename in sorted(glob.glob('frames/frame_*.jpg')):\n",
    "    i += 1\n",
    "\n",
    "    if not i % SAMPLING == 0:  # Sample every Nth\n",
    "        continue\n",
    "\n",
    "    existing_result = tracking_results.get(filename)\n",
    "    if existing_result and existing_result['result'] == 'success':\n",
    "        cache_used += 1\n",
    "    else:\n",
    "        rc, results = detect_objects(filename)\n",
    "        if rc != 200 or results['result'] != 'success':\n",
    "            print(\"ERROR rc=%d for %s\" % (rc, filename))\n",
    "            print(\"ERROR result=%s\" % results)\n",
    "        else:\n",
    "            sampled += 1\n",
    "            # Save frequently to cache partial results\n",
    "            tracking_results[filename] = results\n",
    "            with open('frames/frame-data-newmodel.json', 'w') as fp:\n",
    "                json.dump(tracking_results, fp)\n",
    "\n",
    "    print(\"Processed file {num} of {total_frames} (used cache {cache_used} times)\".format(\n",
    "        num=i, total_frames=total_frames, cache_used=cache_used), end=\"\\r\")\n",
    "\n",
    "# Finally, write all our results\n",
    "with open('frames/frame-data-newmodel.json', 'w') as fp:\n",
    "    json.dump(tracking_results, fp)\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_object(color, textcolor, fontface, image, car, textsize, thickness, xmax, xmid, xmin, ymax, ymid, ymin):\n",
    "    cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, thickness)\n",
    "    pos = (xmid - textsize[0]//2, ymid + textsize[1]//2)\n",
    "    cv2.putText(image, car, pos, fontface, 1, textcolor, thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def update_trackers(image, counters):\n",
    "    left_lane = counters['left_lane']\n",
    "    right_lane = counters['right_lane']\n",
    "    boxes = []\n",
    "    color = (80, 220, 60)\n",
    "    fontface = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontscale = 1\n",
    "    thickness = 1\n",
    "\n",
    "    for n, pair in enumerate(trackers):\n",
    "        tracker, car = pair\n",
    "        textsize, _baseline = cv2.getTextSize(\n",
    "            car, fontface, fontscale, thickness)\n",
    "        success, bbox = tracker.update(image)\n",
    "\n",
    "        if not success:\n",
    "            counters['lost_trackers'] += 1\n",
    "            del trackers[n]\n",
    "            continue\n",
    "\n",
    "        boxes.append(bbox)  # Return updated box list\n",
    "\n",
    "        xmin = int(bbox[0])\n",
    "        ymin = int(bbox[1])\n",
    "        xmax = int(bbox[0] + bbox[2])\n",
    "        ymax = int(bbox[1] + bbox[3])\n",
    "        xmid = int(round((xmin+xmax)/2))\n",
    "        ymid = int(round((ymin+ymax)/2))\n",
    "        \n",
    "        if ymid >= ROI_YMAX:\n",
    "            label_object(WHITE, WHITE, fontface, image, car, textsize, 1, xmax, xmid, xmin, ymax, ymid, ymin)\n",
    "            # Count left-lane, right-lane as cars ymid crosses finish line\n",
    "            if xmid < 630:\n",
    "                left_lane += 1\n",
    "            else:\n",
    "                right_lane += 1\n",
    "            # Stop tracking cars when they hit finish line\n",
    "            del trackers[n]\n",
    "        else:\n",
    "            # Rectangle and number on the cars we are tracking\n",
    "            label_object(color, YELLOW, fontface, image, car, textsize, 4, xmax, xmid, xmin, ymax, ymid, ymin)\n",
    "\n",
    "    # Add finish line overlay/line\n",
    "    overlay = image.copy()\n",
    "\n",
    "    # Shade region of interest (ROI). We're really just using the top line.\n",
    "    cv2.rectangle(overlay,\n",
    "                  (0, ROI_YMAX),\n",
    "                  (FRAME_WIDTH, FRAME_HEIGHT), DARK_BLUE, cv2.FILLED)\n",
    "    cv2.addWeighted(overlay, 0.6, image, 0.4, 0, image)\n",
    "\n",
    "    # Draw start line, if > 0\n",
    "    if START_LINE > 0:\n",
    "        cv2.line(image, (0, START_LINE), (FRAME_WIDTH, START_LINE), GRAY, 4, cv2.LINE_AA)\n",
    "    # Draw finish line with lane hash marks\n",
    "    cv2.line(image, (0, ROI_YMAX), (FRAME_WIDTH, ROI_YMAX), LIGHT_CYAN, 4, cv2.LINE_AA)\n",
    "    cv2.line(image, (350, ROI_YMAX - 20), (350, ROI_YMAX + 20), LIGHT_CYAN, 4, cv2.LINE_AA)\n",
    "    cv2.line(image, (630, ROI_YMAX - 20), (630, ROI_YMAX + 20), LIGHT_CYAN, 4, cv2.LINE_AA)\n",
    "    cv2.line(image, (950, ROI_YMAX - 20), (950, ROI_YMAX + 20), LIGHT_CYAN, 4, cv2.LINE_AA)\n",
    "\n",
    "    # Add lane counter\n",
    "    cv2.putText(image, \"Lane counter:\", (30, ROI_YMAX + 80), fontface, 1.5, LIGHT_CYAN, 4, cv2.LINE_AA)\n",
    "    cv2.putText(image, str(left_lane), (480, ROI_YMAX + 80), fontface, 1.5, LIGHT_CYAN, 4, cv2.LINE_AA)\n",
    "    cv2.putText(image, str(right_lane), (800, ROI_YMAX + 80), fontface, 1.5, LIGHT_CYAN, 4, cv2.LINE_AA)\n",
    "    seconds = counters['frames'] / FRAME_FPS\n",
    "    cv2.putText(image, \"Cars/second:\", (35, ROI_YMAX + 110), fontface, 0.5, LIGHT_CYAN, 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, '{0:.2f}'.format(left_lane / seconds), (480, ROI_YMAX + 110), fontface, 0.5, LIGHT_CYAN, 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, '{0:.2f}'.format(right_lane / seconds), (800, ROI_YMAX + 110), fontface, 0.5, LIGHT_CYAN, 1, cv2.LINE_AA)\n",
    "\n",
    "    counters['left_lane'] = left_lane\n",
    "    counters['right_lane'] = right_lane\n",
    "    return boxes, counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_tracked(objects, boxes):\n",
    "    if not objects:\n",
    "        return []  # No new classified objects to search for\n",
    "    if not boxes:\n",
    "        return objects  # No existing boxes, return all objects\n",
    "\n",
    "    new_objects = []\n",
    "    for obj in objects:\n",
    "        ymin = obj.get(\"ymin\", \"\")\n",
    "        ymax = obj.get(\"ymax\", \"\")\n",
    "        ymid = int(round((ymin+ymax)/2))\n",
    "        xmin = obj.get(\"xmin\", \"\")\n",
    "        xmax = obj.get(\"xmax\", \"\")\n",
    "        xmid = int(round((xmin+xmax)/2))\n",
    "        box_range = ((xmax - xmin) + (ymax - ymin)) / 2\n",
    "        for bbox in boxes:\n",
    "            bxmin = int(bbox[0])\n",
    "            bymin = int(bbox[1])\n",
    "            bxmax = int(bbox[0] + bbox[2])\n",
    "            bymax = int(bbox[1] + bbox[3])\n",
    "            bxmid = int((bxmin + bxmax) / 2)\n",
    "            bymid = int((bymin + bymax) / 2)\n",
    "            if math.sqrt((xmid - bxmid)**2 + (ymid - bymid)**2) < box_range:\n",
    "                # found existing, so break (do not add to new_objects)\n",
    "                break\n",
    "        else:\n",
    "            new_objects.append(obj)\n",
    "\n",
    "    return new_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e202790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_range(obj):\n",
    "    ymin = obj['ymin']\n",
    "    ymax = obj['ymax']\n",
    "    if ymin < START_LINE or ymax > ROI_YMAX:\n",
    "        # Don't add new trackers before start or after finish.\n",
    "        # Start line can help avoid overlaps and tracker loss.\n",
    "        # Finish line protection avoids counting the car twice.\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def add_new_object(obj, image, cars):\n",
    "    car = str(cars)\n",
    "    xmin = obj['xmin']\n",
    "    xmax = obj['xmax']\n",
    "    ymin = obj['ymin']\n",
    "    ymax = obj['ymax']\n",
    "    xmid = int(round((xmin+xmax)/2))\n",
    "    ymid = int(round((ymin+ymax)/2))\n",
    "    fontface = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontscale = 1\n",
    "    thickness = 1\n",
    "    textsize, _baseline = cv2.getTextSize(\n",
    "        car, fontface, fontscale, thickness)\n",
    "\n",
    "    # init tracker\n",
    "    tracker = cv2.TrackerKCF_create()  # Note: Try comparing KCF with MIL\n",
    "    success = tracker.init(image, (xmin, ymin, xmax-xmin, ymax-ymin))\n",
    "    if success:\n",
    "        trackers.append((tracker, car))\n",
    "\n",
    "    label_object(GREEN, YELLOW, fontface, image, car, textsize, 4, xmax, xmid, xmin, ymax, ymid, ymin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fac71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the saved frames and:\n",
    "# Update the trackers to follow already detected objects from frame to frame.\n",
    "# Look for new objects if we ran inference on this frame.\n",
    "# Check for overlap with tracked objects.\n",
    "# If no overlap, assign a sequence number and start tracking.\n",
    "# Write an annotated image with tracked objects highlighted and numbered.\n",
    "\n",
    "cars = 0\n",
    "trackers = []\n",
    "counters = {\n",
    "    'left_lane':  0,\n",
    "    'right_lane':  0,\n",
    "    'lost_trackers': 0,\n",
    "    'frames': 0,\n",
    "}\n",
    "\n",
    "with open('frames/frame-data-newmodel.json') as existing_results:\n",
    "    tracking_results = json.load(existing_results)\n",
    "\n",
    "for filename in sorted(glob.glob('frames/frame_*.jpg')):\n",
    "    counters['frames'] += 1\n",
    "    img = cv2.imread(filename)\n",
    "    boxes, counters = update_trackers(img, counters)\n",
    "\n",
    "    if filename in tracking_results and 'classified' in tracking_results[filename]:\n",
    "        jsonresp = tracking_results[filename]\n",
    "        for obj in not_tracked(jsonresp['classified'], boxes):\n",
    "            if in_range(obj):\n",
    "                cars += 1\n",
    "                add_new_object(obj, img, cars)  # Label and start tracking\n",
    "\n",
    "    # Draw the running total of cars in the image in the upper-left corner\n",
    "    cv2.putText(img, 'Cars detected: ' + str(cars), (30, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, DARK_BLUE, 4, cv2.LINE_AA)\n",
    "    # Add note with count of trackers lost\n",
    "    cv2.putText(img, 'Cars lost: ' + str(counters['lost_trackers']), (35, 85),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, DARK_BLUE, 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imwrite(\"output/output-\" + filename.split('/')[1], img)\n",
    "    print(\"Processed file {num} of {total_frames}\".format(\n",
    "        num=counters['frames'], total_frames=total_frames), end=\"\\r\")\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38835f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will play the annotated frames in a loop to demonstrate the new video\n",
    "for filename in sorted(glob.glob(os.path.join(os.path.abspath(OUTPUT_DIR),\n",
    "                                              'output-frame_*.jpg'))):\n",
    "    frame = cv2.imread(filename)\n",
    "    clear_output(wait=True)\n",
    "    rows, columns, _channels = frame.shape\n",
    "    frame = cv2.resize(frame, (int(columns/2), int(rows/2)))  # shrink it\n",
    "    _ret, jpg = cv2.imencode('.jpg', frame)\n",
    "    display(Image(data=jpg))\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO FRAME PREDICTION:\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, UpSampling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import random\n",
    "import glob\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import subprocess\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow, figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165937fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize wandb and download dataset\n",
    "\n",
    "hyperparams = {\"num_epochs\": 10, \n",
    "          \"batch_size\": 32,\n",
    "          \"height\": 96,\n",
    "          \"width\": 96}\n",
    "\n",
    "wandb.init(config=hyperparams)\n",
    "config = wandb.config\n",
    "\n",
    "val_dir = 'catz/test'\n",
    "train_dir = 'catz/train'\n",
    "\n",
    "# automatically get the data if it doesn't exist\n",
    "if not os.path.exists(\"catz\"):\n",
    "    print(\"Downloading catz dataset...\")\n",
    "    subprocess.check_output(\n",
    "        \"curl https://storage.googleapis.com/wandb/catz.tar.gz | tar xz\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2813d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator to loop over train and test images\n",
    "\n",
    "def my_generator(batch_size, img_dir):\n",
    "    \"\"\"A generator that returns 5 images plus a result image\"\"\"\n",
    "    cat_dirs = glob.glob(img_dir + \"/*\")\n",
    "    counter = 0\n",
    "    while True:\n",
    "        input_images = np.zeros(\n",
    "            (batch_size, config.width, config.height, 3 * 5))\n",
    "        output_images = np.zeros((batch_size, config.width, config.height, 3))\n",
    "        random.shuffle(cat_dirs)\n",
    "        if (counter+batch_size >= len(cat_dirs)):\n",
    "            counter = 0\n",
    "        for i in range(batch_size):\n",
    "            input_imgs = glob.glob(cat_dirs[counter + i] + \"/cat_[0-5]*\")\n",
    "            imgs = [Image.open(img) for img in sorted(input_imgs)]\n",
    "            input_images[i] = np.concatenate(imgs, axis=2)\n",
    "            output_images[i] = np.array(Image.open(\n",
    "                cat_dirs[counter + i] + \"/cat_result.jpg\"))\n",
    "            input_images[i] /= 255.\n",
    "            output_images[i] /= 255.\n",
    "        yield (input_images, output_images)\n",
    "        counter += batch_size\n",
    "        \n",
    "steps_per_epoch = len(glob.glob(train_dir + \"/*\")) // config.batch_size\n",
    "validation_steps = len(glob.glob(val_dir + \"/*\")) // config.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback to log the images\n",
    "\n",
    "class ImageCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        validation_X, validation_y = next(\n",
    "            my_generator(15, val_dir))\n",
    "        output = self.model.predict(validation_X)\n",
    "        wandb.log({\n",
    "            \"input\": [wandb.Image(np.concatenate(np.split(c, 5, axis=2), axis=1)) for c in validation_X],\n",
    "            \"output\": [wandb.Image(np.concatenate([validation_y[i], o], axis=1)) for i, o in enumerate(output)]\n",
    "        }, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the generator\n",
    "gen = my_generator(2, train_dir)\n",
    "videos, next_frame = next(gen)\n",
    "videos[0].shape\n",
    "next_frame[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure()\n",
    "imshow(videos[0][:,:,0:3])\n",
    "figure()\n",
    "imshow(videos[0][:,:,3:6])\n",
    "figure()\n",
    "imshow(videos[0][:,:,6:9])\n",
    "figure()\n",
    "imshow(videos[0][:,:,9:12])\n",
    "\n",
    "figure()\n",
    "imshow(next_frame[0][:,:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for measuring how similar two images are\n",
    "def perceptual_distance(y_true, y_pred):\n",
    "    y_true *= 255.\n",
    "    y_pred *= 255.\n",
    "    rmean = (y_true[:, :, :, 0] + y_pred[:, :, :, 0]) / 2\n",
    "    r = y_true[:, :, :, 0] - y_pred[:, :, :, 0]\n",
    "    g = y_true[:, :, :, 1] - y_pred[:, :, :, 1]\n",
    "    b = y_true[:, :, :, 2] - y_pred[:, :, :, 2]\n",
    "\n",
    "    return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(config=hyperparams)\n",
    "config = wandb.config\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(3, (3, 3), activation='relu', padding='same', input_shape=(config.height, config.width, 5 * 3))) \n",
    "# (3,3)=kernel, padding=same: input size=output size, 5*3=5frames * 3 colours\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[perceptual_distance])\n",
    "# mse=mean squared error\n",
    "\n",
    "model.fit_generator(my_generator(config.batch_size, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=config.num_epochs, callbacks=[\n",
    "    ImageCallback(), WandbCallback()],\n",
    "    validation_steps=validation_steps//4,\n",
    "    validation_data=my_generator(config.batch_size, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model - just return the last layer\n",
    "\n",
    "from keras.layers import Lambda, Reshape, Permute\n",
    "\n",
    "def slice(x):\n",
    "    return x[:,:,:,:, -1]\n",
    "\n",
    "wandb.init(config=hyperparams)\n",
    "config = wandb.config\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Reshape((96,96,5,3), input_shape=(config.height, config.width, 5 * 3)))\n",
    "model.add(Permute((1,2,4,3)))\n",
    "model.add(Lambda(slice, input_shape=(96,96,3,5), output_shape=(96,96,3)))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[perceptual_distance])\n",
    "\n",
    "model.fit_generator(my_generator(config.batch_size, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=config.num_epochs, callbacks=[\n",
    "    ImageCallback(), WandbCallback()],\n",
    "    validation_steps=validation_steps//4,\n",
    "    validation_data=my_generator(config.batch_size, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return the last layer, functional style\n",
    "\n",
    "from keras.layers import Lambda, Reshape, Permute, Input\n",
    "from keras.models import Model\n",
    "\n",
    "def slice(x):\n",
    "    return x[:,:,:,:, -1]\n",
    "\n",
    "wandb.init(config=hyperparams)\n",
    "config = wandb.config\n",
    "\n",
    "inp = Input((config.height, config.width, 5 * 3))\n",
    "reshaped = Reshape((96,96,5,3))(inp)\n",
    "permuted = Permute((1,2,4,3))(reshaped)\n",
    "last_layer = Lambda(slice, input_shape=(96,96,3,5), output_shape=(96,96,3))(permuted)\n",
    "model=Model(inputs=[inp], outputs=[last_layer])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[perceptual_distance])\n",
    "\n",
    "model.fit_generator(my_generator(config.batch_size, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=config.num_epochs, callbacks=[\n",
    "    ImageCallback(), WandbCallback()],\n",
    "    validation_steps=validation_steps//4,\n",
    "    validation_data=my_generator(config.batch_size, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ebea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv3D\n",
    "\n",
    "from keras.layers import Lambda, Reshape, Permute, Input, add, Conv3D\n",
    "from keras.models import Model\n",
    "\n",
    "def slice(x):\n",
    "    return x[:,:,:,:, -1]\n",
    "\n",
    "hyperparams[\"num_epochs\"] = 100\n",
    "wandb.init(config=hyperparams)\n",
    "config = wandb.config\n",
    "\n",
    "inp = Input((config.height, config.width, 5 * 3))\n",
    "reshaped = Reshape((96,96,5,3))(inp)\n",
    "permuted = Permute((1,2,4,3))(reshaped)\n",
    "last_layer = Lambda(slice, input_shape=(96,96,3,5), output_shape=(96,96,3))(permuted)\n",
    "conv_output = Conv3D(1, (3,3,3), padding=\"same\")(permuted)\n",
    "conv_output_reshape = Reshape((96,96,3))(conv_output)\n",
    "combined = add([last_layer, conv_output_reshape])\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[combined])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[perceptual_distance])\n",
    "\n",
    "model.fit_generator(my_generator(config.batch_size, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=config.num_epochs, callbacks=[\n",
    "    ImageCallback(), WandbCallback()],\n",
    "    validation_steps=validation_steps//4,\n",
    "    validation_data=my_generator(config.batch_size, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv3D with Gaussian Noise\n",
    "\n",
    "from keras.layers import Lambda, Reshape, Permute, Input, add, Conv3D, GaussianNoise\n",
    "from keras.models import Model\n",
    "\n",
    "def slice(x):\n",
    "    return x[:,:,:,:, -1]\n",
    "\n",
    "wandb.init()\n",
    "\n",
    "inp = Input((config.height, config.width, 5 * 3))\n",
    "reshaped = Reshape((96,96,5,3))(inp)\n",
    "permuted = Permute((1,2,4,3))(reshaped)\n",
    "noise = GaussianNoise(0.1)(permuted)\n",
    "last_layer = Lambda(slice, input_shape=(96,96,3,5), output_shape=(96,96,3))(noise)\n",
    "conv_output = Conv3D(1, (3,3,3), padding=\"same\")(noise)\n",
    "conv_output_reshape = Reshape((96,96,3))(conv_output)\n",
    "combined = add([last_layer, conv_output_reshape])\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[combined])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[perceptual_distance])\n",
    "\n",
    "model.fit_generator(my_generator(config.batch_size, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=config.num_epochs, callbacks=[\n",
    "    ImageCallback(), WandbCallback()],\n",
    "    validation_steps=validation_steps//4,\n",
    "    validation_data=my_generator(config.batch_size, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2DLSTM with Gaussian Noise\n",
    "\n",
    "from keras.layers import Lambda, Reshape, Permute, Input, add, Conv3D, GaussianNoise, ConvLSTM2D\n",
    "from keras.models import Model\n",
    "\n",
    "def slice(x):\n",
    "    return x[:,:,:,:, -1]\n",
    "\n",
    "wandb.init(config=hyperparams)\n",
    "config = wandb.config\n",
    "\n",
    "inp = Input((config.height, config.width, 5 * 3))\n",
    "reshaped = Reshape((96,96,5,3))(inp)\n",
    "permuted = Permute((1,2,4,3))(reshaped)\n",
    "noise = GaussianNoise(0.1)(permuted)\n",
    "last_layer = Lambda(slice, input_shape=(96,96,3,5), output_shape=(96,96,3))(noise)\n",
    "permuted_2 = Permute((4,1,2,3))(noise)\n",
    "\n",
    "conv_lstm_output_1 = ConvLSTM2D(6, (3,3), padding='same')(permuted_2)\n",
    "conv_output = Conv2D(3, (3,3), padding=\"same\")(conv_lstm_output_1)\n",
    "combined = add([last_layer, conv_output])\n",
    "\n",
    "model=Model(inputs=[inp], outputs=[combined])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[perceptual_distance])\n",
    "\n",
    "model.fit_generator(my_generator(config.batch_size, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=config.num_epochs, callbacks=[\n",
    "    ImageCallback(), WandbCallback()],\n",
    "    validation_steps=validation_steps//4,\n",
    "    validation_data=my_generator(config.batch_size, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e21ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2DLSTM with Gaussian Noise\n",
    "\n",
    "from keras.layers import Lambda, Reshape, Permute, Input, add, Conv3D, GaussianNoise, concatenate\n",
    "from keras.layers import ConvLSTM2D, BatchNormalization, TimeDistributed, Add\n",
    "from keras.models import Model\n",
    "\n",
    "def slice(x):\n",
    "    return x[:,:,:,:, -1]\n",
    "\n",
    "wandb.init(config=hyperparams)\n",
    "config = wandb.config\n",
    "\n",
    "c=4\n",
    "\n",
    "inp = Input((config.height, config.width, 5 * 3))\n",
    "reshaped = Reshape((96,96,5,3))(inp)\n",
    "permuted = Permute((1,2,4,3))(reshaped)\n",
    "noise = GaussianNoise(0.1)(permuted)\n",
    "last_layer = Lambda(slice, input_shape=(96,96,3,5), output_shape=(96,96,3))(noise)\n",
    "x = Permute((4,1,2,3))(noise)\n",
    "x =(ConvLSTM2D(filters=c, kernel_size=(3,3),padding='same',name='conv_lstm1', return_sequences=True))(x)\n",
    "\n",
    "c1=(BatchNormalization())(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x =(TimeDistributed(MaxPooling2D(pool_size=(2,2))))(c1)\n",
    "\n",
    "x =(ConvLSTM2D(filters=2*c,kernel_size=(3,3),padding='same',name='conv_lstm3',return_sequences=True))(x)\n",
    "c2=(BatchNormalization())(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x =(TimeDistributed(MaxPooling2D(pool_size=(2,2))))(c2)\n",
    "x =(ConvLSTM2D(filters=4*c,kernel_size=(3,3),padding='same',name='conv_lstm4',return_sequences=True))(x)\n",
    "\n",
    "x =(TimeDistributed(UpSampling2D(size=(2, 2))))(x)\n",
    "x =(ConvLSTM2D(filters=4*c,kernel_size=(3,3),padding='same',name='conv_lstm5',return_sequences=True))(x)\n",
    "x =(BatchNormalization())(x)\n",
    "\n",
    "x =(ConvLSTM2D(filters=2*c,kernel_size=(3,3),padding='same',name='conv_lstm6',return_sequences=True))(x)\n",
    "x =(BatchNormalization())(x)\n",
    "x = Add()([c2, x])\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x =(TimeDistributed(UpSampling2D(size=(2, 2))))(x)\n",
    "x =(ConvLSTM2D(filters=c,kernel_size=(3,3),padding='same',name='conv_lstm7',return_sequences=False))(x)\n",
    "x =(BatchNormalization())(x)\n",
    "combined = concatenate([last_layer, x])\n",
    "combined = Conv2D(3, (1,1))(combined)\n",
    "model=Model(inputs=[inp], outputs=[combined])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[perceptual_distance])\n",
    "\n",
    "model.fit_generator(my_generator(config.batch_size, train_dir),\n",
    "                    steps_per_epoch=steps_per_epoch//4,\n",
    "                    epochs=config.num_epochs, callbacks=[\n",
    "    ImageCallback(), WandbCallback()],\n",
    "    validation_steps=validation_steps//4,\n",
    "    validation_data=my_generator(config.batch_size, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82125e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
